{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM\n",
    "from transformers import AutoConfig\n",
    "import bigram\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyPixel/small-llama2\")\n",
    "bigram_instance = bigram.bigram(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 59203382\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 14800846\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# DatasetDict({\n",
    "#     train: Dataset({\n",
    "#         features: ['text'],\n",
    "#         num_rows: 74 004 228\n",
    "#     })\n",
    "# })\n",
    "\n",
    "# dataset = load_dataset(\"bookcorpus\")\n",
    "datasets = load_dataset(\"./bookcorpus-splitted\")\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "6000000\n",
      "7000000\n",
      "8000000\n",
      "9000000\n",
      "10000000\n",
      "11000000\n",
      "12000000\n",
      "13000000\n",
      "14000000\n",
      "15000000\n",
      "16000000\n",
      "17000000\n",
      "18000000\n",
      "19000000\n",
      "20000000\n",
      "21000000\n",
      "22000000\n",
      "23000000\n",
      "24000000\n",
      "25000000\n",
      "26000000\n",
      "27000000\n",
      "28000000\n",
      "29000000\n",
      "30000000\n",
      "31000000\n",
      "32000000\n",
      "33000000\n",
      "34000000\n",
      "35000000\n",
      "36000000\n",
      "37000000\n",
      "38000000\n",
      "39000000\n",
      "40000000\n",
      "41000000\n",
      "42000000\n",
      "43000000\n",
      "44000000\n",
      "45000000\n",
      "46000000\n",
      "47000000\n",
      "48000000\n",
      "49000000\n",
      "50000000\n",
      "51000000\n",
      "52000000\n",
      "53000000\n",
      "54000000\n",
      "55000000\n",
      "56000000\n",
      "57000000\n",
      "58000000\n",
      "59000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nDebug:\\n# Error: When chunk_size equals 100,000, the program throws an error: \\n\\nOutput is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\\nthread '<unnamed>' panicked at /home/runner/work/tokenizers/tokenizers/tokenizers/src/models/bpe/word.rs:198:41:\\nindex out of bounds: the len is 6163649 but the index is 274882678541\\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\\n# Solution: decrease chunk_size to 10,000\\n\\n# Error: When chunk_size equals 10,000, the program throws an error: \\nPanicException: index out of bounds: the len is 310549 but the index is 283468065632\\n# Restart the core\\n\\n# Error:\\nPanicException: byte index 274878268401 is out of bounds of `▁dinner was slow cooking , mainly because they were all too listless and exhausted to mess with it . then you toil hard to make a hundred rupee . luke 's mother was at his side . `` that 's my specialty . '' `` it 's final . '' `` i 'm not ashamed of my `[...]\\n\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size = datasets['train'].num_rows\n",
    "chunk_size = 10_000\n",
    "for i in range(int(dataset_size / chunk_size) + 1):\n",
    "    # select data samples, 选择部分数据，数据分块，处理大规模数据\n",
    "    selected_data = datasets['train'].select(range(i * chunk_size, min((i + 1) * chunk_size, dataset_size)))\n",
    "    # print(selected_data) # Dataset\n",
    "    # print(type(selected_data['text'])) # list\n",
    "    if i % 100 == 0:\n",
    "        print(i * chunk_size)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"TinyPixel/small-llama2\")\n",
    "    plain_text = ' '.join(selected_data['text'])\n",
    "    # print(len(plain_text))\n",
    "    # tokenize\n",
    "    ids = tokenizer(plain_text, truncation=False)\n",
    "    # # print(len(ids['input_ids']))\n",
    "    bigram_instance.add_and_count(ids['input_ids'])\n",
    "    # break\n",
    "\n",
    "bigram_probability = bigram_instance.fit()\n",
    "'''\n",
    "Debug:\n",
    "# Error: When chunk_size equals 100,000, the program throws an error: \n",
    "\n",
    "Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\n",
    "thread '<unnamed>' panicked at /home/runner/work/tokenizers/tokenizers/tokenizers/src/models/bpe/word.rs:198:41:\n",
    "index out of bounds: the len is 6163649 but the index is 274882678541\n",
    "note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n",
    "# Solution: decrease chunk_size to 10,000\n",
    "\n",
    "# Error: When chunk_size equals 10,000, the program throws an error: \n",
    "PanicException: index out of bounds: the len is 310549 but the index is 283468065632\n",
    "# Restart the core\n",
    "\n",
    "# Error:\n",
    "PanicException: byte index 274878268401 is out of bounds of `▁dinner was slow cooking , mainly because they were all too listless and exhausted to mess with it . then you toil hard to make a hundred rupee . luke 's mother was at his side . `` that 's my specialty . '' `` it 's final . '' `` i 'm not ashamed of my `[...]\n",
    "# Solution: reboot\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# save data efficiently. @ How to Save a NumPy Array to File for Machine Learning\n",
    "np.save('bigram_probability.npy', bigram_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32000, 32000)\n"
     ]
    }
   ],
   "source": [
    "# load numpy array from npy file\n",
    "from numpy import load\n",
    "# load array\n",
    "data = load('bigram_probability.npy')\n",
    "# print the array\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.99999933, 0.999995  , 0.        ,\n",
       "       0.        , 0.        , 0.99999991, 0.99999999, 0.        ,\n",
       "       0.        , 0.99999995, 0.99999999, 0.        , 0.99999997,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# While broadcasting,  (32000,) behaves like a row vector (1, 32000)\n",
    "np.sum(data,axis=1)[:50]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
