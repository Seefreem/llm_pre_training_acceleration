{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model: small-llama2. https://huggingface.co/TinyPixel/small-llama2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the file that the experiments were conducted on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data\n",
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List, Tuple\n",
    "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch._tensor import Tensor\n",
    "from torch.nn.modules import Module\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.optim.optimizer import Optimizer as Optimizer\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.data.data_collator import DataCollator\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "from transformers.trainer_utils import EvalPrediction\n",
    "from collections.abc import Mapping\n",
    "from transformers.utils import (\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    is_flash_attn_2_available,\n",
    "    is_flash_attn_greater_or_equal_2_10,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    "    CausalLMOutputWithPast,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutputWithPast,\n",
    ")\n",
    "from transformers.models.llama.modeling_llama import LLAMA_INPUTS_DOCSTRING, _CONFIG_FOR_DOC\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING = True\n",
    "training_set_batch = 1_000_000 # 625 training steps. In total 59_203_382 rows\n",
    "test_set_batch = 200_000 # In total 14_800_846 rows\n",
    "bi_gram_scheduling_steps = 0 # 240 # 100, 1 training step is 32 batches\n",
    "bi_gram_weight = 1\n",
    "min_bi_gram_weight = 0.0\n",
    "model_layer_number = 1 # max 12\n",
    "use_bi_gram = False if bi_gram_scheduling_steps == 0 else True\n",
    "teacher_scheduling_method = 'linear' # none, linear, exponential, reciprocal\n",
    "fold_number = 4 # from 0 to 4\n",
    "K = 5 # 5-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 74004228\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "datasets = load_dataset(\"bookcorpus\").shuffle(seed=42)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1000000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 200000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "In total, the dataset has 74004228 rows.\n",
    "74004228 / 5 = 14800845.6\n",
    "Choose 250_000 samples from each fold to construct training dataset.\n",
    "'''\n",
    "from datasets import Dataset\n",
    "new_train_dataset = None\n",
    "new_val_dataset = None\n",
    "for i in range(K):\n",
    "    if i != fold_number:\n",
    "        if new_train_dataset != None:\n",
    "            new_train_dataset.extend(datasets[\"train\"].select(range(i * 14800845, i * 14800845 + 250_000))['text'])\n",
    "        else:\n",
    "            new_train_dataset = datasets[\"train\"].select(range(i * 14800845, i * 14800845 + 250_000))['text']\n",
    "    else:\n",
    "        new_val_dataset = datasets[\"train\"].select(range(i * 14800845, i * 14800845 + test_set_batch))\n",
    "new_train_dataset = Dataset.from_dict({'text': new_train_dataset})\n",
    "datasets = DatasetDict({'train': new_train_dataset, 'validation': new_val_dataset})\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite the forward() function of LlamaForCausalLM, enabling handling soft cross-entropy.\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM\n",
    "class LlamaForCausalLMWithBigram(LlamaForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "        model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        >>> # Generate\n",
    "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
    "        ```\"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "        # print(\"Inside the model:\", attention_mask) # It stays the same\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            logits = torch.cat(logits, dim=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            '''===========modification start point==============='''\n",
    "            # Flatten the tokens\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = None\n",
    "            \n",
    "            if labels.shape[-1] == self.vocab_size:# With bigram: shape (4, 1024, 32000) # Without bigram: torch.Size([4, 1024])\n",
    "                # labels = labels.to_dense()\n",
    "                shift_labels = labels[..., 1:, :].contiguous() # Ignore the first token, it is not a label\n",
    "                # Flatten the tokens\n",
    "                shift_labels = shift_labels.view(-1, self.config.vocab_size)\n",
    "                # print(shift_labels.shape, flush=True)\n",
    "            else:\n",
    "                shift_labels = labels[..., 1:].contiguous()\n",
    "                # Flatten the tokens\n",
    "                shift_labels = shift_labels.view(-1)\n",
    "                # print(shift_labels.shape, flush=True)\n",
    "            '''===========modification end point==============='''\n",
    "                \n",
    "            # Enable model parallelism\n",
    "            # print('Inside the function:', shift_logits.shape, shift_labels.shape)\n",
    "            # print('Inside the function2:', shift_labels.sum())\n",
    "            # print('Inside the function2:', shift_labels)\n",
    "            # '''\n",
    "            # Inside the function: torch.Size([4092, 32000]) torch.Size([4092, 32000])\n",
    "            # Inside the function2: tensor(4092., device='cuda:0', dtype=torch.float64)\n",
    "            # Inside the function2: tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float64)\n",
    "            # '''\n",
    "            \n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLMWithBigram(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=1376, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=1376, bias=False)\n",
       "          (down_proj): Linear(in_features=1376, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyPixel/small-llama2\")\n",
    "tokenizer.pad_token_id=tokenizer.eos_token_id\n",
    "config = AutoConfig.from_pretrained(\"TinyPixel/small-llama2\")\n",
    "config.num_hidden_layers = model_layer_number # originally, 12\n",
    "model = LlamaForCausalLMWithBigram(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53510f7a6fdf49c4a52904acfb170701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 18770\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 3766\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization \n",
    "def tokenize(element):\n",
    "    long_text = \"\".join(element['text']) # concatenation\n",
    "    outputs = tokenizer(\n",
    "        [long_text],\n",
    "        truncation=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "        max_length=config.max_position_embeddings,\n",
    "    )\n",
    "    return {\"input_ids\": outputs['input_ids']}\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize, batched=True, remove_columns=datasets[\"train\"].column_names, \n",
    "    batch_size=200# , num_proc=10\n",
    ")\n",
    "tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32000, 32000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load bi-gram probabilities\n",
    "import numpy\n",
    "bi_gram_probabilities = numpy.load('./bigram_probability.npy').astype(numpy.float16)\n",
    "bi_gram_probabilities.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite _prepare_inputs() of Trainer, enabling teacher-student learning paradigm \n",
    "class MyTrainer(Trainer):\n",
    "    def __init__(self,\n",
    "                model: Union[PreTrainedModel, Module] = None,\n",
    "                args: TrainingArguments = None,\n",
    "                data_collator: Optional[DataCollator] = None,\n",
    "                train_dataset: Optional[Dataset] = None,\n",
    "                eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,\n",
    "                tokenizer: Optional[PreTrainedTokenizerBase] = None,\n",
    "                model_init: Optional[Callable[[], PreTrainedModel]] = None,\n",
    "                compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,\n",
    "                callbacks: Optional[List[TrainerCallback]] = None,\n",
    "                optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),\n",
    "                preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,\n",
    "                bigram_probabilities: Optional[torch.Tensor] = None, # \n",
    "                bi_gram_scheduling_steps: Optional[int] = 0, # \n",
    "                min_bi_gram_weight: Optional[float] = 0.0, # \n",
    "                teacher_scheduling_method = '',\n",
    "                ):\n",
    "        super().__init__(model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\n",
    "        self.bigram_probabilities = bigram_probabilities\n",
    "        self.bi_gram_scheduling_steps = bi_gram_scheduling_steps\n",
    "        self.min_bi_gram_weight = min_bi_gram_weight\n",
    "        self.vocabulary_size = model.vocab_size\n",
    "        self.teacher_scheduling_method = teacher_scheduling_method\n",
    "    \n",
    "    # Not _prepare_input(), recursive calling\n",
    "    def _prepare_inputs(self, inputs: Dict[str, Union[torch.Tensor, Any]]) -> Dict[str, Union[torch.Tensor, Any]]:\n",
    "        \"\"\"\n",
    "        Prepare `inputs` before feeding them to the model, converting them to tensors if they are not already and\n",
    "        handling potential state.\n",
    "        self.control.should_evaluate will be set as True before evaluation, and will be set as False after evaluation.\n",
    "        \"\"\"\n",
    "        '''===========modification start point==============='''\n",
    "        # # Enable min_bi_gram_weight for the training steps after bi-gram scheduling\n",
    "        # if self.bi_gram_scheduling_steps and (not self.control.should_evaluate):\n",
    "        # Disenable min_bi_gram_weight for the training steps after bi-gram scheduling\n",
    "        if self.state.global_step < self.bi_gram_scheduling_steps and (not self.control.should_evaluate): # About 10 times slow than without bigram\n",
    "            labels = inputs['labels'].cpu() # return a copy\n",
    "            # print('labels ', labels.shape) # labels  torch.Size([4, 1024])\n",
    "            # Here is tricky. We should remove then last one of label. \n",
    "            ids = labels.view(-1)\n",
    "            bigram_labels = self.bigram_probabilities[ids[:-1], :]\n",
    "            bigram_weight = 0\n",
    "            if self.teacher_scheduling_method == 'linear': \n",
    "                # ====== linear scheduling ====== \n",
    "                bigram_weight = max(self.min_bi_gram_weight, \n",
    "                                    1 - min(self.state.global_step, self.bi_gram_scheduling_steps) / self.bi_gram_scheduling_steps)\n",
    "            elif self.teacher_scheduling_method == 'exponential':\n",
    "                # ====== exponential scheduling ====== \n",
    "                bigram_weight = np.exp(-self.state.global_step)\n",
    "                # if self.state.global_step % 50 == 49:\n",
    "                #     print(f'The global step is {self.state.global_step}. The bigram weight is {bigram_weight}')\n",
    "                # print('Before ', np.sum(bigram_labels, axis=1)[-10:])\n",
    "                # print('Ids ', ids[-10:])\n",
    "            elif self.teacher_scheduling_method == 'reciprocal':\n",
    "                bigram_weight = 1/(self.state.global_step)\n",
    "            # print('Inside: bigram_weight=', bigram_weight)\n",
    "            # combine bi-gram probabilities and one-hot labels.\n",
    "            bigram_labels *= bigram_weight # Shape  (4096, 32000)\n",
    "            # one_hot_matrix = np.zeros((bigram_labels.shape[0], self.vocabulary_size), dtype=np.float16)\n",
    "            # one_hot_matrix[np.arange(bigram_labels.shape[0]), ids] = 1 - bigram_weight\n",
    "            # bigram_labels[:-1, :] += one_hot_matrix[1:, :]\n",
    "            bigram_labels[np.arange(bigram_labels.shape[0]), ids[1:]] += (1 - bigram_weight)\n",
    "            bigram_labels = np.concatenate((np.zeros((1, self.vocabulary_size)), bigram_labels), 0)\n",
    "            # print('After ', np.sum(bigram_labels, axis=1)[-10:], bigram_labels.shape)\n",
    "            bigram_labels = bigram_labels.reshape((inputs['labels'].shape[0], \n",
    "                                                inputs['labels'].shape[1], \n",
    "                                                self.vocabulary_size)) # Shape  (4, 1024, 32000)\n",
    "            # RuntimeError: Expect the same number of specified elements per batch.\n",
    "            # inputs['labels'] = torch.tensor(bigram_labels).to_sparse_csr().to(self.args.device)\n",
    "            inputs['labels'] = torch.tensor(bigram_labels).to(self.args.device)\n",
    "        '''===========modification end point==============='''\n",
    "        inputs = self._prepare_input(inputs)\n",
    "        if len(inputs) == 0:\n",
    "            raise ValueError(\n",
    "                \"The batch received was empty, your model won't be able to train on it. Double-check that your \"\n",
    "                f\"training dataset contains keys expected by the model: {','.join(self._signature_columns)}.\"\n",
    "            )\n",
    "        if self.args.past_index >= 0 and self._past is not None:\n",
    "            inputs[\"mems\"] = self._past\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shiling/anaconda3/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch import tensor, exp\n",
    "from datetime import datetime\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    print('Inside compute_metrics', eval_pred.predictions.shape, eval_pred.label_ids.shape)\n",
    "    # Inside compute_metrics (11, 1024, 32000) (11, 1024)  numpy.ndarray\n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    prediction = tensor(eval_pred.predictions).view(-1, 32000)\n",
    "    labels = tensor(eval_pred.label_ids).view(-1)\n",
    "    masked_lm_loss = exp(loss_fct(prediction, labels)) \n",
    "    return {'ppl': masked_lm_loss}\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import os\n",
    "# os.environ['WANDB_DISABLED'] = 'true' # turning off reporting to WanDB. It requires API key\n",
    "output_dir=\"llama2-bigram-guided-k-fold\"\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d_%m_%H-%M-%S\") # 27_12_10-09-20\n",
    "log_name = output_dir + '/runs/' + dt_string + (f'_{bi_gram_scheduling_steps}_{min_bi_gram_weight}_{model_layer_number}' \n",
    "                                                if use_bi_gram else f\"_no_bigram_{model_layer_number}\") \n",
    "log_name += \"_\" + teacher_scheduling_method[0] + '_' + str(fold_number)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4, \n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1, \n",
    "    logging_steps=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1, \n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=3, \n",
    "    fp16=True,\n",
    "    push_to_hub=False, \n",
    "    report_to='tensorboard',\n",
    "    logging_dir=log_name\n",
    ")\n",
    "if TRAINING:\n",
    "    args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4, \n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=3_0, \n",
    "        logging_steps=20, \n",
    "        gradient_accumulation_steps=8,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.1,\n",
    "        warmup_steps=1_00, \n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        learning_rate=5e-4,\n",
    "        save_steps=3_000, \n",
    "        fp16=True,\n",
    "        push_to_hub=False,\n",
    "        report_to='tensorboard',\n",
    "        logging_dir=log_name\n",
    "    )\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    # compute_metrics=compute_metrics\n",
    "    bigram_probabilities = bi_gram_probabilities, # The bi-gram probability matrix\n",
    "    bi_gram_scheduling_steps = bi_gram_scheduling_steps, \n",
    "    min_bi_gram_weight = min_bi_gram_weight, # The minimum bi-gram weight\n",
    "    teacher_scheduling_method = teacher_scheduling_method\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f6c7174cec41c78a50d5b0e8f3e2af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.6094, 'grad_norm': 1.9818520545959473, 'learning_rate': 0.0001, 'epoch': 0.03}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23863c1d38e44239959f1d0f6a2a5afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.846191883087158, 'eval_runtime': 34.2766, 'eval_samples_per_second': 109.871, 'eval_steps_per_second': 27.482, 'epoch': 0.05}\n",
      "{'loss': 7.0318, 'grad_norm': 0.5009767413139343, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 6.013, 'grad_norm': 0.530003547668457, 'learning_rate': 0.0003, 'epoch': 0.1}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9afda812ee0e4085abe0be318b9bb472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.714672088623047, 'eval_runtime': 33.8619, 'eval_samples_per_second': 111.217, 'eval_steps_per_second': 27.819, 'epoch': 0.1}\n",
      "{'loss': 5.535, 'grad_norm': 0.5313185453414917, 'learning_rate': 0.0004, 'epoch': 0.14}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ca2a0756c7403ebe80ca39d6d7c6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.282958507537842, 'eval_runtime': 34.0115, 'eval_samples_per_second': 110.727, 'eval_steps_per_second': 27.697, 'epoch': 0.15}\n",
      "{'loss': 5.2913, 'grad_norm': 0.5291732549667358, 'learning_rate': 0.0005, 'epoch': 0.17}\n",
      "{'loss': 5.1479, 'grad_norm': 0.5427552461624146, 'learning_rate': 0.0004979136257327503, 'epoch': 0.2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f212df5df9e94237b143f6c5ebb47073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.080759525299072, 'eval_runtime': 34.4618, 'eval_samples_per_second': 109.28, 'eval_steps_per_second': 27.335, 'epoch': 0.2}\n",
      "{'loss': 5.0386, 'grad_norm': 0.4574202001094818, 'learning_rate': 0.0004916893265916655, 'epoch': 0.24}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574e8be183834684a33b2275ac972f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.963342666625977, 'eval_runtime': 34.6616, 'eval_samples_per_second': 108.65, 'eval_steps_per_second': 27.177, 'epoch': 0.26}\n",
      "{'loss': 4.9674, 'grad_norm': 0.45416349172592163, 'learning_rate': 0.00048143099231722267, 'epoch': 0.27}\n",
      "{'loss': 4.891, 'grad_norm': 0.4451713263988495, 'learning_rate': 0.00046730984470666194, 'epoch': 0.31}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1f378492014d109a4fbe984af4cac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.874007225036621, 'eval_runtime': 34.741, 'eval_samples_per_second': 108.402, 'eval_steps_per_second': 27.115, 'epoch': 0.31}\n",
      "{'loss': 4.8534, 'grad_norm': 0.4192894995212555, 'learning_rate': 0.0004495615797519732, 'epoch': 0.34}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b88f616f424c828753e2a27cccb792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.800921440124512, 'eval_runtime': 35.0272, 'eval_samples_per_second': 107.516, 'eval_steps_per_second': 26.893, 'epoch': 0.36}\n",
      "{'loss': 4.8106, 'grad_norm': 0.4221283793449402, 'learning_rate': 0.0004284824336394748, 'epoch': 0.38}\n",
      "{'loss': 4.7661, 'grad_norm': 0.4034672677516937, 'learning_rate': 0.00040442423827336427, 'epoch': 0.41}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb71a39892e443f38f8d7b81b07bae26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.740621566772461, 'eval_runtime': 34.7013, 'eval_samples_per_second': 108.526, 'eval_steps_per_second': 27.146, 'epoch': 0.41}\n",
      "{'loss': 4.7326, 'grad_norm': 0.40580856800079346, 'learning_rate': 0.0003777885488514683, 'epoch': 0.44}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8005db1426cc43dfba8306bb9048ef78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.6825456619262695, 'eval_runtime': 34.2534, 'eval_samples_per_second': 109.945, 'eval_steps_per_second': 27.501, 'epoch': 0.46}\n",
      "{'loss': 4.6818, 'grad_norm': 0.4124316871166229, 'learning_rate': 0.00034901994150978924, 'epoch': 0.48}\n",
      "{'loss': 4.6374, 'grad_norm': 0.4120369553565979, 'learning_rate': 0.0003185985929048254, 'epoch': 0.51}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b543eda890405aac0187f372c38d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.627112865447998, 'eval_runtime': 34.2272, 'eval_samples_per_second': 110.029, 'eval_steps_per_second': 27.522, 'epoch': 0.51}\n",
      "{'loss': 4.6044, 'grad_norm': 0.386046439409256, 'learning_rate': 0.00028703226558781227, 'epoch': 0.55}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7181514c0941528e2cfcea13ed3a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.582106113433838, 'eval_runtime': 34.2469, 'eval_samples_per_second': 109.966, 'eval_steps_per_second': 27.506, 'epoch': 0.56}\n",
      "{'loss': 4.5874, 'grad_norm': 0.4245465099811554, 'learning_rate': 0.0002548478329429561, 'epoch': 0.58}\n",
      "{'loss': 4.5581, 'grad_norm': 0.4106554388999939, 'learning_rate': 0.0002225824851468671, 'epoch': 0.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ffcf0e3cd342239e66e6adb627b012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.541588306427002, 'eval_runtime': 34.4898, 'eval_samples_per_second': 109.192, 'eval_steps_per_second': 27.312, 'epoch': 0.61}\n",
      "{'loss': 4.5367, 'grad_norm': 0.37997713685035706, 'learning_rate': 0.00019077476293047018, 'epoch': 0.65}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0faa94c7a8ad495db8abab61e189c923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.508094787597656, 'eval_runtime': 34.7334, 'eval_samples_per_second': 108.426, 'eval_steps_per_second': 27.121, 'epoch': 0.66}\n",
      "{'loss': 4.5034, 'grad_norm': 0.3847604990005493, 'learning_rate': 0.00015995556879882245, 'epoch': 0.68}\n",
      "{'loss': 4.4908, 'grad_norm': 0.4286229610443115, 'learning_rate': 0.00013063930574051273, 'epoch': 0.72}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a3b36c164e495a94259c74ca348bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.479612827301025, 'eval_runtime': 35.307, 'eval_samples_per_second': 106.664, 'eval_steps_per_second': 26.68, 'epoch': 0.72}\n",
      "{'loss': 4.4827, 'grad_norm': 0.4362623691558838, 'learning_rate': 0.00010331529133039555, 'epoch': 0.75}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8e9ab751944b698687e96a5a43cad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.458536148071289, 'eval_runtime': 34.9189, 'eval_samples_per_second': 107.85, 'eval_steps_per_second': 26.977, 'epoch': 0.77}\n",
      "{'loss': 4.4561, 'grad_norm': 0.3950788974761963, 'learning_rate': 7.843959053281663e-05, 'epoch': 0.78}\n",
      "{'loss': 4.4576, 'grad_norm': 0.4128142297267914, 'learning_rate': 5.6427403523966886e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "782be2bbe6264fbe831eca2534621b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.442216396331787, 'eval_runtime': 34.9424, 'eval_samples_per_second': 107.777, 'eval_steps_per_second': 26.959, 'epoch': 0.82}\n",
      "{'loss': 4.4397, 'grad_norm': 0.36803099513053894, 'learning_rate': 3.7646135588175675e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ddbd8346a1e43d08fce545fe220fe3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.430988311767578, 'eval_runtime': 35.1233, 'eval_samples_per_second': 107.222, 'eval_steps_per_second': 26.82, 'epoch': 0.87}\n",
      "{'loss': 4.4339, 'grad_norm': 0.40709859132766724, 'learning_rate': 2.240926475846336e-05, 'epoch': 0.89}\n",
      "{'loss': 4.4284, 'grad_norm': 0.37404724955558777, 'learning_rate': 1.0971109556530106e-05, 'epoch': 0.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8deb0a06c5854259b50c5295bbd35447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.425398826599121, 'eval_runtime': 34.8804, 'eval_samples_per_second': 107.969, 'eval_steps_per_second': 27.007, 'epoch': 0.92}\n",
      "{'loss': 4.4223, 'grad_norm': 0.3711010217666626, 'learning_rate': 3.5225841638008847e-06, 'epoch': 0.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce20ddd2715b4bd19363a01862429028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.423478603363037, 'eval_runtime': 34.4342, 'eval_samples_per_second': 109.368, 'eval_steps_per_second': 27.357, 'epoch': 0.97}\n",
      "{'loss': 4.4198, 'grad_norm': 0.3579881191253662, 'learning_rate': 1.8801187394248964e-07, 'epoch': 0.99}\n",
      "{'train_runtime': 1178.7105, 'train_samples_per_second': 15.924, 'train_steps_per_second': 0.497, 'train_loss': 4.988185898842665, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=586, training_loss=4.988185898842665, metrics={'train_runtime': 1178.7105, 'train_samples_per_second': 15.924, 'train_steps_per_second': 0.497, 'train_loss': 4.988185898842665, 'epoch': 1.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.system(\"shutdown -t  60 \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
