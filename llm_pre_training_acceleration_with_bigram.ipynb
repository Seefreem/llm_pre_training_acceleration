{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model: small-llama2. https://huggingface.co/TinyPixel/small-llama2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the file that the experiments were conducted on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data\n",
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List, Tuple\n",
    "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch._tensor import Tensor\n",
    "from torch.nn.modules import Module\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.optim.optimizer import Optimizer as Optimizer\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.data.data_collator import DataCollator\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "from transformers.trainer_utils import EvalPrediction\n",
    "from collections.abc import Mapping\n",
    "from transformers.utils import (\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    is_flash_attn_2_available,\n",
    "    is_flash_attn_greater_or_equal_2_10,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    "    CausalLMOutputWithPast,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutputWithPast,\n",
    ")\n",
    "from transformers.models.llama.modeling_llama import LLAMA_INPUTS_DOCSTRING, _CONFIG_FOR_DOC\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING = True\n",
    "training_set_batch = 1_000_000 # 625 training steps. In total 59_203_382 rows\n",
    "test_set_batch = 200_000 # In total 14_800_846 rows\n",
    "bi_gram_scheduling_steps = 240 # 100, 1 training step is 32 batches\n",
    "bi_gram_weight = 1\n",
    "min_bi_gram_weight = 0.0\n",
    "model_layer_number = 1 # max 12\n",
    "use_bi_gram = False if bi_gram_scheduling_steps == 0 else True\n",
    "teacher_scheduling_method = 'linear' # none, linear, exponential, reciprocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1000000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 200000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "datasets = load_dataset(\"./bookcorpus-splitted\")\n",
    "# datasets.cleanup_cache_files() # clean GPU cache\n",
    "datasets = DatasetDict({'train': datasets['train'].select(range(training_set_batch)), \n",
    "            'validation': datasets['validation'].select(range(test_set_batch))})\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite the forward() function of LlamaForCausalLM, enabling handling soft cross-entropy.\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM\n",
    "class LlamaForCausalLMWithBigram(LlamaForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "        model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        >>> # Generate\n",
    "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
    "        ```\"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "        # print(\"Inside the model:\", attention_mask) # It stays the same\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            logits = torch.cat(logits, dim=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            '''===========modification start point==============='''\n",
    "            # Flatten the tokens\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = None\n",
    "            \n",
    "            if labels.shape[-1] == self.vocab_size:# With bigram: shape (4, 1024, 32000) # Without bigram: torch.Size([4, 1024])\n",
    "                # labels = labels.to_dense()\n",
    "                shift_labels = labels[..., 1:, :].contiguous() # Ignore the first token, it is not a label\n",
    "                # Flatten the tokens\n",
    "                shift_labels = shift_labels.view(-1, self.config.vocab_size)\n",
    "                # print(shift_labels.shape, flush=True)\n",
    "            else:\n",
    "                shift_labels = labels[..., 1:].contiguous()\n",
    "                # Flatten the tokens\n",
    "                shift_labels = shift_labels.view(-1)\n",
    "                # print(shift_labels.shape, flush=True)\n",
    "            '''===========modification end point==============='''\n",
    "                \n",
    "            # Enable model parallelism\n",
    "            # print('Inside the function:', shift_logits.shape, shift_labels.shape)\n",
    "            # print('Inside the function2:', shift_labels.sum())\n",
    "            # print('Inside the function2:', shift_labels)\n",
    "            # '''\n",
    "            # Inside the function: torch.Size([4092, 32000]) torch.Size([4092, 32000])\n",
    "            # Inside the function2: tensor(4092., device='cuda:0', dtype=torch.float64)\n",
    "            # Inside the function2: tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float64)\n",
    "            # '''\n",
    "            \n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLMWithBigram(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-4): 5 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=1376, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=1376, bias=False)\n",
       "          (down_proj): Linear(in_features=1376, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyPixel/small-llama2\")\n",
    "tokenizer.pad_token_id=tokenizer.eos_token_id\n",
    "config = AutoConfig.from_pretrained(\"TinyPixel/small-llama2\")\n",
    "config.num_hidden_layers = model_layer_number # originally, 12\n",
    "model = LlamaForCausalLMWithBigram(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b0064e93f14c27afb317e8ce41ac36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b4a53b0eb4424696f456951762e7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 18823\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 3750\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization \n",
    "def tokenize(element):\n",
    "    long_text = \"\".join(element['text']) # concatenation\n",
    "    outputs = tokenizer(\n",
    "        [long_text],\n",
    "        truncation=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "        max_length=config.max_position_embeddings,\n",
    "    )\n",
    "    return {\"input_ids\": outputs['input_ids']}\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize, batched=True, remove_columns=datasets[\"train\"].column_names, \n",
    "    batch_size=200# , num_proc=10\n",
    ")\n",
    "tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32000, 32000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load bi-gram probabilities\n",
    "import numpy\n",
    "bi_gram_probabilities = numpy.load('./bigram_probability.npy').astype(numpy.float16)\n",
    "bi_gram_probabilities.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite _prepare_inputs() of Trainer, enabling teacher-student learning paradigm \n",
    "class MyTrainer(Trainer):\n",
    "    def __init__(self,\n",
    "                model: Union[PreTrainedModel, Module] = None,\n",
    "                args: TrainingArguments = None,\n",
    "                data_collator: Optional[DataCollator] = None,\n",
    "                train_dataset: Optional[Dataset] = None,\n",
    "                eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,\n",
    "                tokenizer: Optional[PreTrainedTokenizerBase] = None,\n",
    "                model_init: Optional[Callable[[], PreTrainedModel]] = None,\n",
    "                compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,\n",
    "                callbacks: Optional[List[TrainerCallback]] = None,\n",
    "                optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),\n",
    "                preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,\n",
    "                bigram_probabilities: Optional[torch.Tensor] = None, # \n",
    "                bi_gram_scheduling_steps: Optional[int] = 0, # \n",
    "                min_bi_gram_weight: Optional[float] = 0.0, # \n",
    "                teacher_scheduling_method = '',\n",
    "                ):\n",
    "        super().__init__(model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\n",
    "        self.bigram_probabilities = bigram_probabilities\n",
    "        self.bi_gram_scheduling_steps = bi_gram_scheduling_steps\n",
    "        self.min_bi_gram_weight = min_bi_gram_weight\n",
    "        self.vocabulary_size = model.vocab_size\n",
    "        self.teacher_scheduling_method = teacher_scheduling_method\n",
    "    \n",
    "    # Not _prepare_input(), recursive calling\n",
    "    def _prepare_inputs(self, inputs: Dict[str, Union[torch.Tensor, Any]]) -> Dict[str, Union[torch.Tensor, Any]]:\n",
    "        \"\"\"\n",
    "        Prepare `inputs` before feeding them to the model, converting them to tensors if they are not already and\n",
    "        handling potential state.\n",
    "        self.control.should_evaluate will be set as True before evaluation, and will be set as False after evaluation.\n",
    "        \"\"\"\n",
    "        '''===========modification start point==============='''\n",
    "        # # Enable min_bi_gram_weight for the training steps after bi-gram scheduling\n",
    "        # if self.bi_gram_scheduling_steps and (not self.control.should_evaluate):\n",
    "        # Disenable min_bi_gram_weight for the training steps after bi-gram scheduling\n",
    "        if self.state.global_step < self.bi_gram_scheduling_steps and (not self.control.should_evaluate): # About 10 times slow than without bigram\n",
    "            labels = inputs['labels'].cpu() # return a copy\n",
    "            # print('labels ', labels.shape) # labels  torch.Size([4, 1024])\n",
    "            # Here is tricky. We should remove then last one of label. \n",
    "            ids = labels.view(-1)\n",
    "            bigram_labels = self.bigram_probabilities[ids[:-1], :]\n",
    "            bigram_weight = 0\n",
    "            if self.teacher_scheduling_method == 'linear': \n",
    "                # ====== linear scheduling ====== \n",
    "                bigram_weight = max(self.min_bi_gram_weight, \n",
    "                                    1 - min(self.state.global_step, self.bi_gram_scheduling_steps) / self.bi_gram_scheduling_steps)\n",
    "            elif self.teacher_scheduling_method == 'exponential':\n",
    "                # ====== exponential scheduling ====== \n",
    "                bigram_weight = np.exp(-self.state.global_step)\n",
    "                # if self.state.global_step % 50 == 49:\n",
    "                #     print(f'The global step is {self.state.global_step}. The bigram weight is {bigram_weight}')\n",
    "                # print('Before ', np.sum(bigram_labels, axis=1)[-10:])\n",
    "                # print('Ids ', ids[-10:])\n",
    "            elif self.teacher_scheduling_method == 'reciprocal':\n",
    "                bigram_weight = 1/(self.state.global_step)\n",
    "            # print('Inside: bigram_weight=', bigram_weight)\n",
    "            # combine bi-gram probabilities and one-hot labels.\n",
    "            bigram_labels *= bigram_weight # Shape  (4096, 32000)\n",
    "            # one_hot_matrix = np.zeros((bigram_labels.shape[0], self.vocabulary_size), dtype=np.float16)\n",
    "            # one_hot_matrix[np.arange(bigram_labels.shape[0]), ids] = 1 - bigram_weight\n",
    "            # bigram_labels[:-1, :] += one_hot_matrix[1:, :]\n",
    "            bigram_labels[np.arange(bigram_labels.shape[0]), ids[1:]] += (1 - bigram_weight)\n",
    "            bigram_labels = np.concatenate((np.zeros((1, self.vocabulary_size)), bigram_labels), 0)\n",
    "            # print('After ', np.sum(bigram_labels, axis=1)[-10:], bigram_labels.shape)\n",
    "            bigram_labels = bigram_labels.reshape((inputs['labels'].shape[0], \n",
    "                                                inputs['labels'].shape[1], \n",
    "                                                self.vocabulary_size)) # Shape  (4, 1024, 32000)\n",
    "            # RuntimeError: Expect the same number of specified elements per batch.\n",
    "            # inputs['labels'] = torch.tensor(bigram_labels).to_sparse_csr().to(self.args.device)\n",
    "            inputs['labels'] = torch.tensor(bigram_labels).to(self.args.device)\n",
    "        '''===========modification end point==============='''\n",
    "        inputs = self._prepare_input(inputs)\n",
    "        if len(inputs) == 0:\n",
    "            raise ValueError(\n",
    "                \"The batch received was empty, your model won't be able to train on it. Double-check that your \"\n",
    "                f\"training dataset contains keys expected by the model: {','.join(self._signature_columns)}.\"\n",
    "            )\n",
    "        if self.args.past_index >= 0 and self._past is not None:\n",
    "            inputs[\"mems\"] = self._past\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shiling/anaconda3/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch import tensor, exp\n",
    "from datetime import datetime\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    print('Inside compute_metrics', eval_pred.predictions.shape, eval_pred.label_ids.shape)\n",
    "    # Inside compute_metrics (11, 1024, 32000) (11, 1024)  numpy.ndarray\n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    prediction = tensor(eval_pred.predictions).view(-1, 32000)\n",
    "    labels = tensor(eval_pred.label_ids).view(-1)\n",
    "    masked_lm_loss = exp(loss_fct(prediction, labels)) \n",
    "    return {'ppl': masked_lm_loss}\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import os\n",
    "# os.environ['WANDB_DISABLED'] = 'true' # turning off reporting to WanDB. It requires API key\n",
    "output_dir=\"llama2-tiny-bigram-guided\"\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d_%m_%H-%M-%S\") # 27_12_10-09-20\n",
    "log_name = output_dir + '/runs/' + dt_string + (f'_{bi_gram_scheduling_steps}_{min_bi_gram_weight}_{model_layer_number}' \n",
    "                                                if use_bi_gram else f\"_no_bigram_{model_layer_number}\") \n",
    "log_name += \"_\" + teacher_scheduling_method[0]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4, \n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1, \n",
    "    logging_steps=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1, \n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=3, \n",
    "    fp16=True,\n",
    "    push_to_hub=False, \n",
    "    report_to='tensorboard',\n",
    "    logging_dir=log_name\n",
    ")\n",
    "if TRAINING:\n",
    "    args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4, \n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=3_0, \n",
    "        logging_steps=20, \n",
    "        gradient_accumulation_steps=8,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.1,\n",
    "        warmup_steps=1_00, \n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        learning_rate=5e-4,\n",
    "        save_steps=3_000, \n",
    "        fp16=True,\n",
    "        push_to_hub=False,\n",
    "        report_to='tensorboard',\n",
    "        logging_dir=log_name\n",
    "    )\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    # compute_metrics=compute_metrics\n",
    "    bigram_probabilities = bi_gram_probabilities, # The bi-gram probability matrix\n",
    "    bi_gram_scheduling_steps = bi_gram_scheduling_steps, \n",
    "    min_bi_gram_weight = min_bi_gram_weight, # The minimum bi-gram weight\n",
    "    teacher_scheduling_method = teacher_scheduling_method\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3e05bc2d314a0bb5dfc03af70c6835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/588 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.2807, 'grad_norm': 1.7205259799957275, 'learning_rate': 0.0001, 'epoch': 0.03}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c8d1eaf2b144f19fbc60aa7e3f7ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.870324611663818, 'eval_runtime': 63.3847, 'eval_samples_per_second': 59.163, 'eval_steps_per_second': 14.799, 'epoch': 0.05}\n",
      "{'loss': 7.044, 'grad_norm': 1.2406632900238037, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 6.0816, 'grad_norm': 0.8223330974578857, 'learning_rate': 0.0003, 'epoch': 0.1}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968b845cd4ec4752a0bcfa0f045fd465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.797129154205322, 'eval_runtime': 64.0235, 'eval_samples_per_second': 58.572, 'eval_steps_per_second': 14.651, 'epoch': 0.1}\n",
      "{'loss': 5.5693, 'grad_norm': 0.6335774064064026, 'learning_rate': 0.0004, 'epoch': 0.14}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f409b9758d924527b2eb24d47f610d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.22218656539917, 'eval_runtime': 65.3327, 'eval_samples_per_second': 57.399, 'eval_steps_per_second': 14.357, 'epoch': 0.15}\n",
      "{'loss': 5.2478, 'grad_norm': 0.7424453496932983, 'learning_rate': 0.0005, 'epoch': 0.17}\n",
      "{'loss': 5.0245, 'grad_norm': 0.5844109058380127, 'learning_rate': 0.0004979306685340254, 'epoch': 0.2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b4324968364d99a645a5cfb0790038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.919995307922363, 'eval_runtime': 65.2324, 'eval_samples_per_second': 57.487, 'eval_steps_per_second': 14.379, 'epoch': 0.2}\n",
      "{'loss': 4.8586, 'grad_norm': 0.554875373840332, 'learning_rate': 0.0004917569311978301, 'epoch': 0.24}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5c278f63d64d5caee914cd3c8c8612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.730818748474121, 'eval_runtime': 64.44, 'eval_samples_per_second': 58.194, 'eval_steps_per_second': 14.556, 'epoch': 0.25}\n",
      "{'loss': 4.7476, 'grad_norm': 0.5016651749610901, 'learning_rate': 0.00048158099206287375, 'epoch': 0.27}\n",
      "{'loss': 4.6482, 'grad_norm': 0.4443685710430145, 'learning_rate': 0.00046757131025753886, 'epoch': 0.31}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7202df7ca8547079cebac89a360d2d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.604829788208008, 'eval_runtime': 64.4612, 'eval_samples_per_second': 58.175, 'eval_steps_per_second': 14.551, 'epoch': 0.31}\n",
      "{'loss': 4.5654, 'grad_norm': 0.49049606919288635, 'learning_rate': 0.0004499598111849299, 'epoch': 0.34}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f63c33e76a4bc287fdaa9b06d1179c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.508819103240967, 'eval_runtime': 65.2058, 'eval_samples_per_second': 57.51, 'eval_steps_per_second': 14.385, 'epoch': 0.36}\n",
      "{'loss': 4.5161, 'grad_norm': 0.4793235957622528, 'learning_rate': 0.0004290380470785983, 'epoch': 0.37}\n",
      "{'loss': 4.4636, 'grad_norm': 0.4683516323566437, 'learning_rate': 0.0004051523704568557, 'epoch': 0.41}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d33a1bb4e841c4a38199b067df1e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.42645263671875, 'eval_runtime': 64.6231, 'eval_samples_per_second': 58.029, 'eval_steps_per_second': 14.515, 'epoch': 0.41}\n",
      "{'loss': 4.4152, 'grad_norm': 0.5146693587303162, 'learning_rate': 0.00037869820037745775, 'epoch': 0.44}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d110a0f9424383b8e9bbf23b258c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.364832401275635, 'eval_runtime': 64.3108, 'eval_samples_per_second': 58.311, 'eval_steps_per_second': 14.585, 'epoch': 0.46}\n",
      "{'loss': 4.3784, 'grad_norm': 0.4118437170982361, 'learning_rate': 0.0003501134764128167, 'epoch': 0.48}\n",
      "{'loss': 4.3342, 'grad_norm': 0.39648815989494324, 'learning_rate': 0.0003198714087129024, 'epoch': 0.51}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f79c693e2a44cd0bff557cf8a585184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.306804656982422, 'eval_runtime': 65.1709, 'eval_samples_per_second': 57.541, 'eval_steps_per_second': 14.393, 'epoch': 0.51}\n",
      "{'loss': 4.2854, 'grad_norm': 0.40555375814437866, 'learning_rate': 0.0002884726441760155, 'epoch': 0.54}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f17f4e70d94d4c7380ed19b5c217d8b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.258877754211426, 'eval_runtime': 65.0735, 'eval_samples_per_second': 57.627, 'eval_steps_per_second': 14.414, 'epoch': 0.56}\n",
      "{'loss': 4.2619, 'grad_norm': 0.4265148341655731, 'learning_rate': 0.0002564369784137472, 'epoch': 0.58}\n",
      "{'loss': 4.2451, 'grad_norm': 0.36791926622390747, 'learning_rate': 0.00022429475071565987, 'epoch': 0.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7abef7d41dee4337ae5faf6bce860ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.2207465171813965, 'eval_runtime': 64.4333, 'eval_samples_per_second': 58.2, 'eval_steps_per_second': 14.558, 'epoch': 0.61}\n",
      "{'loss': 4.2181, 'grad_norm': 0.3923856019973755, 'learning_rate': 0.00019257806446705113, 'epoch': 0.65}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b09c7471e34e86a09f466f55428954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.182222366333008, 'eval_runtime': 64.1214, 'eval_samples_per_second': 58.483, 'eval_steps_per_second': 14.629, 'epoch': 0.66}\n",
      "{'loss': 4.2031, 'grad_norm': 0.40806257724761963, 'learning_rate': 0.0001618119783627263, 'epoch': 0.68}\n",
      "{'loss': 4.1582, 'grad_norm': 0.38628047704696655, 'learning_rate': 0.0001325058142431701, 'epoch': 0.71}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7737ece21cec42868af3f8510f53e0d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.152008533477783, 'eval_runtime': 64.658, 'eval_samples_per_second': 57.997, 'eval_steps_per_second': 14.507, 'epoch': 0.71}\n",
      "{'loss': 4.1593, 'grad_norm': 0.40800344944000244, 'learning_rate': 0.00010514472544885909, 'epoch': 0.75}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7042dd58bb48d0959587193b7a7823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.1272501945495605, 'eval_runtime': 64.8984, 'eval_samples_per_second': 57.783, 'eval_steps_per_second': 14.453, 'epoch': 0.76}\n",
      "{'loss': 4.1289, 'grad_norm': 0.3945483863353729, 'learning_rate': 8.018166527567672e-05, 'epoch': 0.78}\n",
      "{'loss': 4.1248, 'grad_norm': 0.3674790561199188, 'learning_rate': 5.802988849085e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b1c64c3d93146629370b0ee7c98b159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.106303691864014, 'eval_runtime': 65.1958, 'eval_samples_per_second': 57.519, 'eval_steps_per_second': 14.387, 'epoch': 0.82}\n",
      "{'loss': 4.1076, 'grad_norm': 0.40021446347236633, 'learning_rate': 3.905611004420359e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1724ef1bd84782982945bbb2ce699e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.09246826171875, 'eval_runtime': 65.1437, 'eval_samples_per_second': 57.565, 'eval_steps_per_second': 14.399, 'epoch': 0.87}\n",
      "{'loss': 4.0963, 'grad_norm': 0.34703195095062256, 'learning_rate': 2.3574434229882145e-05, 'epoch': 0.88}\n",
      "{'loss': 4.0898, 'grad_norm': 0.38489699363708496, 'learning_rate': 1.1841154799154375e-05, 'epoch': 0.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88adba48181f4770b40e6deb9dee5b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.084216594696045, 'eval_runtime': 65.1797, 'eval_samples_per_second': 57.533, 'eval_steps_per_second': 14.391, 'epoch': 0.92}\n",
      "{'loss': 4.1002, 'grad_norm': 0.3758687973022461, 'learning_rate': 4.0505121066209125e-06, 'epoch': 0.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6033bcd1714b579bcd201cfc89f618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.08123779296875, 'eval_runtime': 64.2768, 'eval_samples_per_second': 58.341, 'eval_steps_per_second': 14.593, 'epoch': 0.97}\n",
      "{'loss': 4.0841, 'grad_norm': 0.37361082434654236, 'learning_rate': 3.314775287923677e-07, 'epoch': 0.99}\n",
      "{'train_runtime': 2331.2378, 'train_samples_per_second': 8.074, 'train_steps_per_second': 0.252, 'train_loss': 4.730655728554239, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=588, training_loss=4.730655728554239, metrics={'train_runtime': 2331.2378, 'train_samples_per_second': 8.074, 'train_steps_per_second': 0.252, 'train_loss': 4.730655728554239, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.system(\"shutdown -t  60 \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
